{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_snapshot_data(log_file):\n",
    "    # Step 1: Read the log file\n",
    "    with open(log_file, 'r') as file:\n",
    "        log_data = file.read()\n",
    "\n",
    "    # Step 2: Define regex patterns for both types of strings\n",
    "    pattern_snapshot = re.compile(\n",
    "        r'Iteration (?P<iteration>\\d+) '\n",
    "        r'dp_(?P<dp>\\d+)_pp_(?P<pp>\\d+)_tp_(?P<tp>\\d+) '\n",
    "        r'snapshot time: (?P<snapshot_time>[0-9.]+), '\n",
    "        r'snapshot_size: (?P<snapshot_size>[0-9.]+) MB, '\n",
    "        r'snapshot_speed: (?P<snapshot_speed>[0-9.]+) MB/s'\n",
    "    )\n",
    "\n",
    "    pattern_throughput = re.compile(\n",
    "        r'dp_(?P<dp>\\d+)_pp_(?P<pp>\\d+)_tp_(?P<tp>\\d+) '\n",
    "        r'throughput_metrics: (?P<throughput_metrics>\\{.*?\\})'\n",
    "    )\n",
    "\n",
    "    # Step 3: Find all matches for both patterns\n",
    "    matches_snapshot = list(pattern_snapshot.finditer(log_data))\n",
    "    matches_throughput = list(pattern_throughput.finditer(log_data))\n",
    "\n",
    "    # Step 4: Extract data and store in a dictionary\n",
    "    snapshot_data = {}\n",
    "    \n",
    "    # Process snapshot matches\n",
    "    for match in matches_snapshot:\n",
    "        dp = match.group('dp')\n",
    "        pp = match.group('pp')\n",
    "        tp = match.group('tp')\n",
    "        iteration = int(match.group('iteration'))\n",
    "        key = f'dp_{dp}_pp_{pp}_tp_{tp}'\n",
    "\n",
    "        if key not in snapshot_data:\n",
    "            snapshot_data[key] = {}\n",
    "\n",
    "        snapshot_data[key][iteration] = {\n",
    "            'snapshot_time': float(match.group('snapshot_time')),\n",
    "            'snapshot_size': float(match.group('snapshot_size')),\n",
    "            'snapshot_speed': float(match.group('snapshot_speed'))\n",
    "        }\n",
    "\n",
    "    # Process throughput matches\n",
    "    for match in matches_throughput:\n",
    "        dp = match.group('dp')\n",
    "        pp = match.group('pp')\n",
    "        tp = match.group('tp')\n",
    "        throughput_metrics_str = match.group('throughput_metrics')\n",
    "\n",
    "        try:\n",
    "            throughput_metrics = ast.literal_eval(throughput_metrics_str)\n",
    "            iteration = throughput_metrics['throughput/iteration']\n",
    "            key = f'dp_{dp}_pp_{pp}_tp_{tp}'\n",
    "\n",
    "            if key not in snapshot_data:\n",
    "                snapshot_data[key] = {}\n",
    "\n",
    "            if iteration not in snapshot_data[key]:\n",
    "                snapshot_data[key][iteration] = {}\n",
    "\n",
    "            snapshot_data[key][iteration].update({\n",
    "                'iteration_time': throughput_metrics['throughput/iteration-time'],\n",
    "                'samples_per_sec': throughput_metrics['throughput/samples_per_sec'],\n",
    "                'tflops': throughput_metrics['throughput/tflops'],\n",
    "            })\n",
    "        except (ValueError, SyntaxError, KeyError) as e:\n",
    "            print(f\"Error parsing throughput metrics: {e}\")\n",
    "            print(f\"Throughput metrics string: {throughput_metrics_str}\")\n",
    "\n",
    "    return snapshot_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_experiment_logs(log_folder, experiment_mapping):\n",
    "    experiment_data = {}\n",
    "\n",
    "    # Traverse through the subfolders\n",
    "    for subfolder in os.listdir(log_folder):\n",
    "        subfolder_path = os.path.join(log_folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # Find the .log file within the subfolder\n",
    "            log_file = None\n",
    "            for file_name in os.listdir(subfolder_path):\n",
    "                if file_name.endswith('.log'):\n",
    "                    log_file = os.path.join(subfolder_path, file_name)\n",
    "                    break\n",
    "\n",
    "            if not log_file:\n",
    "                continue\n",
    "\n",
    "            # Determine the experiment name from the log file name using the mapping\n",
    "            experiment_name = None\n",
    "            for key, name in experiment_mapping.items():\n",
    "                if key in log_file:\n",
    "                    experiment_name = name\n",
    "                    # Remove this item\n",
    "                    del experiment_mapping[key]\n",
    "                    break\n",
    "\n",
    "            if not experiment_name:\n",
    "                continue\n",
    "\n",
    "            # Extract data from the log file\n",
    "            snapshot_data = extract_snapshot_data(log_file)\n",
    "\n",
    "            # Organize the extracted data by experiment and combination of dp, pp, tp\n",
    "            if experiment_name not in experiment_data:\n",
    "                experiment_data[experiment_name] = {}\n",
    "\n",
    "            for key, iterations in snapshot_data.items():\n",
    "                if key not in experiment_data[experiment_name]:\n",
    "                    experiment_data[experiment_name][key] = []\n",
    "\n",
    "                for iteration, data in iterations.items():\n",
    "                    if (iteration > 5 and iteration < 15) or iteration > 22:\n",
    "                        experiment_data[experiment_name][key].append(data)\n",
    "\n",
    "    # Calculate mean values for each experiment and combination of dp, pp, tp\n",
    "    mean_values = {}\n",
    "    overall_means = {}\n",
    "    for experiment, data_dict in experiment_data.items():\n",
    "        experiment_combined_data = []\n",
    "        mean_values[experiment] = {}\n",
    "        for key, data_list in data_dict.items():\n",
    "            df = pd.DataFrame(data_list)\n",
    "            mean_dict = df.mean(numeric_only=True).to_dict()\n",
    "            # Calculate snapshot_speed using averaged snapshot_size and snapshot_time\n",
    "            if 'snapshot_size' in mean_dict and 'snapshot_time' in mean_dict and mean_dict['snapshot_time'] != 0:\n",
    "                mean_dict['snapshot_speed'] = mean_dict['snapshot_size'] / mean_dict['snapshot_time']\n",
    "            mean_values[experiment][key] = mean_dict\n",
    "            experiment_combined_data.extend(data_list)\n",
    "        \n",
    "        # Calculate overall mean for the experiment\n",
    "        combined_df = pd.DataFrame(experiment_combined_data)\n",
    "        overall_mean_dict = combined_df.mean(numeric_only=True).to_dict()\n",
    "        # Calculate snapshot_speed using averaged snapshot_size and snapshot_time\n",
    "        if 'snapshot_size' in overall_mean_dict and 'snapshot_time' in overall_mean_dict and overall_mean_dict['snapshot_time'] != 0:\n",
    "            overall_mean_dict['snapshot_speed'] = overall_mean_dict['snapshot_size'] / overall_mean_dict['snapshot_time']\n",
    "        overall_means[experiment] = overall_mean_dict\n",
    "\n",
    "    return overall_means\n",
    "\n",
    "\n",
    "def get_experiment_values(experiment_name, dp_pp_tp, processed_values):\n",
    "    experiment_data = processed_values.get(experiment_name, None)\n",
    "    if experiment_data:\n",
    "        return experiment_data.get(dp_pp_tp, None)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data_to_csv(mean_values, csv_file_name, dp_pp_tp=None):\n",
    "    # Flatten the hierarchical dictionary into a list of dictionaries\n",
    "    flattened_data = []\n",
    "    for experiment, data_dict in mean_values.items():\n",
    "        row = {'experiment': experiment}\n",
    "        row.update(data_dict)\n",
    "        flattened_data.append(row)\n",
    "\n",
    "    # Create a DataFrame from the flattened data\n",
    "    df = pd.DataFrame(flattened_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(csv_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak scaling of batch size\n",
    "\n",
    "import pprint\n",
    "log_folder = \"/hpc2hdd/home/zli755/xueze/reft_ds/Megatron-DeepSpeed/examples_deepspeed/data_efficiency/gpt/output/log\"\n",
    "\n",
    "experiment_mapping = {  \"23.38\": \"strong_scaling-no_snapshot-no_sharding-dp_1-sync_record\",\n",
    "                        \"23.58\": \"strong_scaling-sync_snapshot-no_sharding-dp_1-sync_record\",\n",
    "                        \"00.13\": \"strong_scaling-pure_thread-no_sharding-dp_1-sync_record\",\n",
    "                        \"00.20\": \"strong_scaling-thread-no_sharding-dp_1-sync_record\",\n",
    "                        \"00.26\": \"strong_scaling-thread_stream-no_sharding-dp_1-sync_record\",\n",
    "                        \"23.49\": \"strong_scaling-bubble-no_sharding-dp_1-sync_record\",\n",
    "                        \"09.24\": \"strong_scaling-no_snapshot-no_sharding-dp_2-sync_record\", \n",
    "                        \"09.34\": \"strong_scaling-sync_snapshot-no_sharding-dp_2-sync_record\",\n",
    "                        \"09.47\": \"strong_scaling-pure_thread-no_sharding-dp_2-sync_record\",\n",
    "                        \"18.31\": \"strong_scaling-thread-no_sharding-dp_2-sync_record\",\n",
    "                        \"10.09\": \"strong_scaling-thread_stream-no_sharding-dp_2-sync_record\",\n",
    "                        \"10.13\": \"strong_scaling-bubble-no_sharding-dp_2-sync_record\",\n",
    "                        \"12.19\": \"strong_scaling-no_snapshot-no_sharding-dp_4-sync_record\",\n",
    "                        \"12.24\": \"strong_scaling-sync_snapshot-no_sharding-dp_4-sync_record\",\n",
    "                        \"12.45\": \"strong_scaling-pure_thread-no_sharding-dp_4-sync_record\",\n",
    "                        \"12.56\": \"strong_scaling-thread-no_sharding-dp_4-sync_record\",\n",
    "                        \"13.03\": \"strong_scaling-thread_stream-no_sharding-dp_4-sync_record\",\n",
    "                        \"13.08\": \"strong_scaling-bubble-no_sharding-dp_4-sync_record\", \n",
    "                        \"18.49\": \"strong_scaling-pure_thread-sharding-dp_2-sync_record\",\n",
    "                        \"10.42\": \"strong_scaling-thread-sharding-dp_2-sync_record\",\n",
    "                        \"10.49\": \"strong_scaling-thread_stream-sharding-dp_2-sync_record\",\n",
    "                        \"11.25\": \"strong_scaling-bubble-sharding-dp_2-sync_record\",\n",
    "                        \"13.33\": \"strong_scaling-pure_thread-sharding-dp_4-sync_record\", \n",
    "                        \"13.42\": \"strong_scaling-thread-sharding-dp_4-sync_record\", \n",
    "                        \"13.48\": \"strong_scaling-thread_stream-sharding-dp_4-sync_record\", \n",
    "                        \"13.55\": \"strong_scaling-bubble-sharding-dp_4-sync_record\", \n",
    "                        \"00.37\": \"strong_scaling-thread_stream-no_sharding-dp_1-no_sync_record\",\n",
    "                        \"04.02\": \"strong_scaling-bubble-no_sharding-dp_1-no_sync_record\",\n",
    "                        \"10.20\": \"strong_scaling-thread_stream-no_sharding-dp_2-no_sync_record\", \n",
    "                        \"10.25\": \"strong_scaling-bubble-no_sharding-dp_2-no_sync_record\", \n",
    "                        \"13.18\": \"strong_scaling-thread_stream-no_sharding-dp_4-no_sync_record\", \n",
    "                        \"13.27\": \"strong_scaling-bubble-no_sharding-dp_4-no_sync_record\", \n",
    "                        \"11.29\": \"strong_scaling-thread_stream-sharding-dp_2-no_sync_record\", \n",
    "                        \"11.32\": \"strong_scaling-bubble-sharding-dp_2-no_sync_record\", \n",
    "                        \"13.59\": \"strong_scaling-thread_stream-sharding-dp_4-no_sync_record\", \n",
    "                        \"14.03\": \"strong_scaling-bubble-sharding-dp_4-no_sync_record\"}\n",
    "processed_data_strong_scaling = process_experiment_logs(log_folder, experiment_mapping)\n",
    "\n",
    "# pprint.pprint(processed_data_strong_scaling)\n",
    "\n",
    "csv_file_name = \"/hpc2hdd/home/zli755/xueze/reft_ds/Megatron-DeepSpeed/examples_deepspeed/data_efficiency/gpt/data_process/processed_data/strong_scaling.csv\"\n",
    "save_processed_data_to_csv(processed_data_strong_scaling, csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weak_scaling_bs-13B-bubble-no_sharding-pp_16-bs_2': {'iteration_time': 3.382360321633956,\n",
      "                                                       'samples_per_sec': 4.730697916441348,\n",
      "                                                       'tflops': 49.46547382839215},\n",
      " 'weak_scaling_bs-13B-no_snapshot-no_sharding-pp_16-bs_2': {'iteration_time': 3.5804108048186585,\n",
      "                                                            'samples_per_sec': 4.539332446166725,\n",
      "                                                            'tflops': 47.46450402885609},\n",
      " 'weak_scaling_bs-13B-pure_thread-no_sharding-pp_16-bs_2': {'iteration_time': 10.170005672118243,\n",
      "                                                            'samples_per_sec': 1.5983936179321443,\n",
      "                                                            'tflops': 16.713241697488932},\n",
      " 'weak_scaling_bs-13B-sync_snapshot-no_sharding-pp_16-bs_2': {'iteration_time': 23.146857473780127,\n",
      "                                                              'samples_per_sec': 0.6915818291039005,\n",
      "                                                              'tflops': 7.2313691281865875},\n",
      " 'weak_scaling_bs-13B-thread-no_sharding-pp_16-bs_2': {'iteration_time': 4.709044122520615,\n",
      "                                                       'samples_per_sec': 3.403025754296629,\n",
      "                                                       'tflops': 35.58296986190397},\n",
      " 'weak_scaling_bs-13B-thread_stream-no_sharding-pp_16-bs_2': {'iteration_time': 4.5199110788457535,\n",
      "                                                              'samples_per_sec': 3.552200739667401,\n",
      "                                                              'tflops': 37.14278438928337},\n",
      " 'weak_scaling_bs-34B-bubble-no_sharding-pp_32-bs_2': {'iteration_time': 5.416344260029933,\n",
      "                                                       'samples_per_sec': 2.9571517480688585,\n",
      "                                                       'tflops': 38.36395162844621},\n",
      " 'weak_scaling_bs-34B-no_snapshot-no_sharding-pp_32-bs_2': {'iteration_time': 5.472644820809364,\n",
      "                                                            'samples_per_sec': 2.9384074174228294,\n",
      "                                                            'tflops': 38.12077621660558},\n",
      " 'weak_scaling_bs-34B-pure_thread-no_sharding-pp_32-bs_2': {'iteration_time': 14.605453592889448,\n",
      "                                                            'samples_per_sec': 1.1040493703721772,\n",
      "                                                            'tflops': 14.323139374918686},\n",
      " 'weak_scaling_bs-34B-sync_snapshot-no_sharding-pp_32-bs_2': {'iteration_time': 39.29787735553349,\n",
      "                                                              'samples_per_sec': 0.40769278603778975,\n",
      "                                                              'tflops': 5.289111839808106},\n",
      " 'weak_scaling_bs-34B-thread-no_sharding-pp_32-bs_2': {'iteration_time': 6.412041360841078,\n",
      "                                                       'samples_per_sec': 2.4966957648535697,\n",
      "                                                       'tflops': 32.39032816504569},\n",
      " 'weak_scaling_bs-34B-thread_stream-no_sharding-pp_32-bs_2': {'iteration_time': 6.405976829721647,\n",
      "                                                              'samples_per_sec': 2.501961655764735,\n",
      "                                                              'tflops': 32.45864403159822},\n",
      " 'weak_scaling_bs-7B-bubble-no_sharding-pp_8-bs_2': {'iteration_time': 3.024638072532766,\n",
      "                                                     'samples_per_sec': 5.296842778009928,\n",
      "                                                     'tflops': 58.04069725742753},\n",
      " 'weak_scaling_bs-7B-no_snapshot-no_sharding-pp_8-bs_2': {'iteration_time': 3.2767958150190464,\n",
      "                                                          'samples_per_sec': 5.007055191293387,\n",
      "                                                          'tflops': 54.86532009512962},\n",
      " 'weak_scaling_bs-7B-pure_thread-no_sharding-pp_8-bs_2': {'iteration_time': 8.331663682180292,\n",
      "                                                          'samples_per_sec': 2.0431899812447236,\n",
      "                                                          'tflops': 22.38846348869518},\n",
      " 'weak_scaling_bs-7B-sync_snapshot-no_sharding-pp_8-bs_2': {'iteration_time': 19.511987840428073,\n",
      "                                                            'samples_per_sec': 0.820956380950489,\n",
      "                                                            'tflops': 8.995713628903061},\n",
      " 'weak_scaling_bs-7B-thread-no_sharding-pp_8-bs_2': {'iteration_time': 4.328094513977275,\n",
      "                                                     'samples_per_sec': 3.7525434911581264,\n",
      "                                                     'tflops': 41.11887965031662},\n",
      " 'weak_scaling_bs-7B-thread_stream-no_sharding-pp_8-bs_2': {'iteration_time': 4.298363641780965,\n",
      "                                                            'samples_per_sec': 3.7882896532821726,\n",
      "                                                            'tflops': 41.51057188301229}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# Weak scaling of model size\n",
    "\n",
    "log_folder = \"/hpc2hdd/home/zli755/yuhan/reft_ds/Megatron-DeepSpeed/examples_deepspeed/data_efficiency/gpt/output/log\"\n",
    "experiment_mapping = {\n",
    "    \"05.20_19.18\": \"weak_scaling_bs-7B-bubble-no_sharding-pp_8-bs_2\",\n",
    "    \"05.21_20.39\": \"weak_scaling_bs-7B-no_snapshot-no_sharding-pp_8-bs_2\",\n",
    "    \"05.21_20.43\": \"weak_scaling_bs-7B-sync_snapshot-no_sharding-pp_8-bs_2\",\n",
    "    \"05.21_21.01\": \"weak_scaling_bs-7B-thread-no_sharding-pp_8-bs_2\",\n",
    "    \"05.21_20.55\": \"weak_scaling_bs-7B-thread_stream-no_sharding-pp_8-bs_2\",\n",
    "    \"05.21_21.28\": \"weak_scaling_bs-7B-pure_thread-no_sharding-pp_8-bs_2\",\n",
    "    \"05.20_23.25\": \"weak_scaling_bs-13B-bubble-no_sharding-pp_16-bs_2\",\n",
    "    \"05.21_20.23\": \"weak_scaling_bs-13B-no_snapshot-no_sharding-pp_16-bs_2\",\n",
    "    \"05.21_20.08\": \"weak_scaling_bs-13B-sync_snapshot-no_sharding-pp_16-bs_2\",\n",
    "    \"05.21_19.55\": \"weak_scaling_bs-13B-thread-no_sharding-pp_16-bs_2\",\n",
    "    \"05.21_20.02\": \"weak_scaling_bs-13B-thread_stream-no_sharding-pp_16-bs_2\",\n",
    "    \"05.21_22.05\": \"weak_scaling_bs-13B-pure_thread-no_sharding-pp_16-bs_2\",\n",
    "    \"05.21_18.29\": \"weak_scaling_bs-34B-bubble-no_sharding-pp_32-bs_2\",\n",
    "    \"05.21_18.41\": \"weak_scaling_bs-34B-no_snapshot-no_sharding-pp_32-bs_2\",\n",
    "    \"05.21_18.49\": \"weak_scaling_bs-34B-sync_snapshot-no_sharding-pp_32-bs_2\",\n",
    "    \"05.21_19.44\": \"weak_scaling_bs-34B-thread-no_sharding-pp_32-bs_2\",\n",
    "    \"05.21_19.38\": \"weak_scaling_bs-34B-thread_stream-no_sharding-pp_32-bs_2\",\n",
    "    \"05.21_21.52\": \"weak_scaling_bs-34B-pure_thread-no_sharding-pp_32-bs_2\",\n",
    "}\n",
    "processed_data_weak_scaling_bs = process_experiment_logs(log_folder, experiment_mapping)\n",
    "\n",
    "pprint.pprint(processed_data_weak_scaling_bs)\n",
    "\n",
    "csv_file_name = \"/hpc2hdd/home/zli755/xueze/reft_ds/Megatron-DeepSpeed/examples_deepspeed/data_efficiency/gpt/data_process/processed_data/weak_scaling_bs.csv\"\n",
    "save_processed_data_to_csv(processed_data_weak_scaling_bs, csv_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
