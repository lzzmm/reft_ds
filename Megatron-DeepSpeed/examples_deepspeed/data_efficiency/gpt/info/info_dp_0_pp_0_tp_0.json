tied_modules.embed.word_embeddings.weight: torch.Size([25216, 768])
tied_modules.embed.position_embeddings.weight: torch.Size([2048, 768])
2.input_layernorm.weight: torch.Size([768])
2.input_layernorm.bias: torch.Size([768])
2.self_attention.query_key_value.weight: torch.Size([1152, 768])
2.self_attention.query_key_value.bias: torch.Size([1152])
2.self_attention.dense.weight: torch.Size([768, 384])
2.self_attention.dense.bias: torch.Size([768])
2.post_attention_layernorm.weight: torch.Size([768])
2.post_attention_layernorm.bias: torch.Size([768])
2.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
2.mlp.dense_h_to_4h.bias: torch.Size([1536])
2.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
2.mlp.dense_4h_to_h.bias: torch.Size([768])
3.input_layernorm.weight: torch.Size([768])
3.input_layernorm.bias: torch.Size([768])
3.self_attention.query_key_value.weight: torch.Size([1152, 768])
3.self_attention.query_key_value.bias: torch.Size([1152])
3.self_attention.dense.weight: torch.Size([768, 384])
3.self_attention.dense.bias: torch.Size([768])
3.post_attention_layernorm.weight: torch.Size([768])
3.post_attention_layernorm.bias: torch.Size([768])
3.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
3.mlp.dense_h_to_4h.bias: torch.Size([1536])
3.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
3.mlp.dense_4h_to_h.bias: torch.Size([768])
4.input_layernorm.weight: torch.Size([768])
4.input_layernorm.bias: torch.Size([768])
4.self_attention.query_key_value.weight: torch.Size([1152, 768])
4.self_attention.query_key_value.bias: torch.Size([1152])
4.self_attention.dense.weight: torch.Size([768, 384])
4.self_attention.dense.bias: torch.Size([768])
4.post_attention_layernorm.weight: torch.Size([768])
4.post_attention_layernorm.bias: torch.Size([768])
4.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
4.mlp.dense_h_to_4h.bias: torch.Size([1536])
4.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
4.mlp.dense_4h_to_h.bias: torch.Size([768])
5.input_layernorm.weight: torch.Size([768])
5.input_layernorm.bias: torch.Size([768])
5.self_attention.query_key_value.weight: torch.Size([1152, 768])
5.self_attention.query_key_value.bias: torch.Size([1152])
5.self_attention.dense.weight: torch.Size([768, 384])
5.self_attention.dense.bias: torch.Size([768])
5.post_attention_layernorm.weight: torch.Size([768])
5.post_attention_layernorm.bias: torch.Size([768])
5.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
5.mlp.dense_h_to_4h.bias: torch.Size([1536])
5.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
5.mlp.dense_4h_to_h.bias: torch.Size([768])
6.input_layernorm.weight: torch.Size([768])
6.input_layernorm.bias: torch.Size([768])
6.self_attention.query_key_value.weight: torch.Size([1152, 768])
6.self_attention.query_key_value.bias: torch.Size([1152])
6.self_attention.dense.weight: torch.Size([768, 384])
6.self_attention.dense.bias: torch.Size([768])
6.post_attention_layernorm.weight: torch.Size([768])
6.post_attention_layernorm.bias: torch.Size([768])
6.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
6.mlp.dense_h_to_4h.bias: torch.Size([1536])
6.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
6.mlp.dense_4h_to_h.bias: torch.Size([768])
7.input_layernorm.weight: torch.Size([768])
7.input_layernorm.bias: torch.Size([768])
7.self_attention.query_key_value.weight: torch.Size([1152, 768])
7.self_attention.query_key_value.bias: torch.Size([1152])
7.self_attention.dense.weight: torch.Size([768, 384])
7.self_attention.dense.bias: torch.Size([768])
7.post_attention_layernorm.weight: torch.Size([768])
7.post_attention_layernorm.bias: torch.Size([768])
7.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
7.mlp.dense_h_to_4h.bias: torch.Size([1536])
7.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
7.mlp.dense_4h_to_h.bias: torch.Size([768])
