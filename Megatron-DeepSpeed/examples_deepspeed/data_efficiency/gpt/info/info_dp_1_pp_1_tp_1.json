tied_modules.embed.word_embeddings.weight: torch.Size([25216, 768])
tied_modules.embed.position_embeddings.weight: torch.Size([2048, 768])
8.input_layernorm.weight: torch.Size([768])
8.input_layernorm.bias: torch.Size([768])
8.self_attention.query_key_value.weight: torch.Size([1152, 768])
8.self_attention.query_key_value.bias: torch.Size([1152])
8.self_attention.dense.weight: torch.Size([768, 384])
8.self_attention.dense.bias: torch.Size([768])
8.post_attention_layernorm.weight: torch.Size([768])
8.post_attention_layernorm.bias: torch.Size([768])
8.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
8.mlp.dense_h_to_4h.bias: torch.Size([1536])
8.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
8.mlp.dense_4h_to_h.bias: torch.Size([768])
9.input_layernorm.weight: torch.Size([768])
9.input_layernorm.bias: torch.Size([768])
9.self_attention.query_key_value.weight: torch.Size([1152, 768])
9.self_attention.query_key_value.bias: torch.Size([1152])
9.self_attention.dense.weight: torch.Size([768, 384])
9.self_attention.dense.bias: torch.Size([768])
9.post_attention_layernorm.weight: torch.Size([768])
9.post_attention_layernorm.bias: torch.Size([768])
9.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
9.mlp.dense_h_to_4h.bias: torch.Size([1536])
9.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
9.mlp.dense_4h_to_h.bias: torch.Size([768])
10.input_layernorm.weight: torch.Size([768])
10.input_layernorm.bias: torch.Size([768])
10.self_attention.query_key_value.weight: torch.Size([1152, 768])
10.self_attention.query_key_value.bias: torch.Size([1152])
10.self_attention.dense.weight: torch.Size([768, 384])
10.self_attention.dense.bias: torch.Size([768])
10.post_attention_layernorm.weight: torch.Size([768])
10.post_attention_layernorm.bias: torch.Size([768])
10.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
10.mlp.dense_h_to_4h.bias: torch.Size([1536])
10.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
10.mlp.dense_4h_to_h.bias: torch.Size([768])
11.input_layernorm.weight: torch.Size([768])
11.input_layernorm.bias: torch.Size([768])
11.self_attention.query_key_value.weight: torch.Size([1152, 768])
11.self_attention.query_key_value.bias: torch.Size([1152])
11.self_attention.dense.weight: torch.Size([768, 384])
11.self_attention.dense.bias: torch.Size([768])
11.post_attention_layernorm.weight: torch.Size([768])
11.post_attention_layernorm.bias: torch.Size([768])
11.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
11.mlp.dense_h_to_4h.bias: torch.Size([1536])
11.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
11.mlp.dense_4h_to_h.bias: torch.Size([768])
12.input_layernorm.weight: torch.Size([768])
12.input_layernorm.bias: torch.Size([768])
12.self_attention.query_key_value.weight: torch.Size([1152, 768])
12.self_attention.query_key_value.bias: torch.Size([1152])
12.self_attention.dense.weight: torch.Size([768, 384])
12.self_attention.dense.bias: torch.Size([768])
12.post_attention_layernorm.weight: torch.Size([768])
12.post_attention_layernorm.bias: torch.Size([768])
12.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
12.mlp.dense_h_to_4h.bias: torch.Size([1536])
12.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
12.mlp.dense_4h_to_h.bias: torch.Size([768])
13.input_layernorm.weight: torch.Size([768])
13.input_layernorm.bias: torch.Size([768])
13.self_attention.query_key_value.weight: torch.Size([1152, 768])
13.self_attention.query_key_value.bias: torch.Size([1152])
13.self_attention.dense.weight: torch.Size([768, 384])
13.self_attention.dense.bias: torch.Size([768])
13.post_attention_layernorm.weight: torch.Size([768])
13.post_attention_layernorm.bias: torch.Size([768])
13.mlp.dense_h_to_4h.weight: torch.Size([1536, 768])
13.mlp.dense_h_to_4h.bias: torch.Size([1536])
13.mlp.dense_4h_to_h.weight: torch.Size([768, 1536])
13.mlp.dense_4h_to_h.bias: torch.Size([768])
14.weight: torch.Size([768])
14.bias: torch.Size([768])
